{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pickle\nimport torch\n\n# Get the interactive Tools for Matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [9.5, 6]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the vocabulary computed by the CBOW Preprocessing notebook (cbow-preprocessing), and the word vectors computed by the CBOW Training notebook (cbow-vectors)"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_VERSION = 'ca-100'\nCBOW_VOCABULARY_ROOT = f'../input/cbow-preprocessing/data/{DATASET_VERSION}'\nCBOW_VECTORS_ROOT = f'../input/cbow-training-v-1/data/{DATASET_VERSION}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict = f'{CBOW_VOCABULARY_ROOT}/ca.wiki.train.tokens.nopunct.dic'\ncounter = pickle.load(open(dict, 'rb'))\nwords, values = zip(*counter.most_common(5000))\nprint('Most frequent Catalan words')\nprint(words[:10])\nprint(values[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Zipf's law of words**. Zipf's law was originally formulated in terms of quantitative linguistics, stating that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table."},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.plot(values[:50], 'g', 2*values[0]/np.arange(2,52), 'r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.loglog(values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Benford's law**, also called the Newcomb–Benford law, the law of anomalous numbers, or the first-digit law, is an observation about the frequency distribution of leading digits in many real-life sets of numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nbenford = Counter(int(str(item[1])[0]) for item in counter.most_common(5000))\nprint(benford)\npercentage = np.array(list(benford.values()), dtype=np.float)\npercentage /= percentage.sum()\n_ = plt.bar(list(benford.keys()), percentage*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelname = f'{CBOW_VECTORS_ROOT}/{DATASET_VERSION}_c.pt'\nstate_dict = torch.load(modelname, map_location=torch.device('cpu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_word_vectors = state_dict['emb.weight'].numpy()\noutput_word_vectors = state_dict['lin.weight'].numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_vocab = Vocabulary()\ntoken_vocab.load(f'{CBOW_VOCABULARY_ROOT}/ca.wiki.vocab')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class WordVectors:\n    def __init__(self, vectors, vocabulary):\n        \"\"\"\n        \"\"\" \n        self.vocabulary = vocabulary\n        self.vectors = vectors\n    \n    def most_similar(self, word, topn=10):\n        \"\"\"\n        \"\"\"\n        tokens = list()\n        similarities = list()\n        \n        if type(word) == str:\n            word_embedding = self.get_word_embedding(word)       \n        else:\n            word_embedding = word\n            \n        for i, token in enumerate(self.vocabulary.token2idx):\n            \n            token_embedding = self.get_word_embedding(token)\n            similarity = ( np.dot(word_embedding, token_embedding) /\n                           (np.linalg.norm(word_embedding)*np.linalg.norm(token_embedding)) )\n\n            if i < topn:\n                tokens.append(token)\n                similarities.append(similarity)\n            \n            elif similarity > min(similarities):\n                replace_idx = similarities.index(min(similarities))\n                tokens[replace_idx] = token\n                similarities[replace_idx] = similarity \n    \n        return sorted(list(zip(tokens, similarities)), key=lambda x: -x[1])\n            \n    \n    \n    def analogy(self, x1, x2, y1, topn=5, keep_all=False):\n        \"\"\"\n        \"\"\"\n        x1_emb = self.get_word_embedding(x1)\n        x2_emb = self.get_word_embedding(x2)\n        y1_emb = self.get_word_embedding(y1)\n        analogy_emb = y1_emb + (x2_emb - x1_emb)\n        \n        analogies = self.most_similar(analogy_emb, topn+3)\n        if not keep_all:\n            analogies = [(k, v) for k,v in analogies if k not in (x1, x2, y1)]\n        return analogies[:topn]\n    \n    \n    def get_word_embedding(self, word):\n        \"\"\"\n        \"\"\"\n        word_idx = self.vocabulary.token2idx[word]\n        return self.vectors[word_idx]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = WordVectors(input_word_vectors, token_vocab)\nmodel2 = WordVectors(output_word_vectors, token_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[x[0] for x in model1.most_similar('Joan', topn=6)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.analogy('França', 'francès', 'Polònia', topn=)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.most_similar('Joan')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.most_similar('lleidatà',100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.most_similar('feminisme', 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.most_similar('justícia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}